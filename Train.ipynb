{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFM_Reconstrucción.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5d1kb44-v-g"
      },
      "source": [
        "# **RECONSTUCTION GAN**\n",
        "\n",
        "En este cuaderno se va a realizar la reconstruccion imagenes que han sido previamente dañadas para obtener a partir de estas y gracias a una Red Generativa las imagenes originales. Para ello se va a usar el modelo Pix2Pix basado en el siguiente paper https://arxiv.org/abs/1611.07004\n",
        "\n",
        "Esto se ha realizado para el trabajo de fin de master y obtencion del titulo de ingeniero en telecomunicaciones por la universidad de cantabria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCeGq2GAl83g"
      },
      "source": [
        "# Se importan las librerias\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqMtO8ZsP-ip"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tqa2filxY9v",
        "outputId": "d3471c5b-1817-425f-f88b-ae1df20408ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38dbX7mgUK0g"
      },
      "source": [
        "# **CARGA DE DATOS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEkBn-0imFIY"
      },
      "source": [
        "# Images path\n",
        "PATH =                  # main path\n",
        "PATH_RESULTS =          # results path\n",
        "INPATH =                # input images path\n",
        "OUPATH =                # hr images path\n",
        "CKPATH =                # google colab checkpoints path\n",
        "\n",
        "# Se listan las fotos de entrada, las urls\n",
        "imgurls = !ls -1 \"{INPATH}\"\n",
        "# Numero de imagenes que se van a usar\n",
        "n = 1077                            # En funcion de las imagenes que me pase Adolfo <-------------------\n",
        "# Usaré para entrenamiento el 80%\n",
        "train_n = round(n*0.8)\n",
        "# Se randomiza el listado de imagenes\n",
        "randurls = np.copy(imgurls)\n",
        "# Se barajean las url de las imagenes\n",
        "np.random.seed()\n",
        "np.random.shuffle(randurls)\n",
        "# Se separan en bloque de train y de test\n",
        "tr_urls = randurls[:train_n]\n",
        "ts_urls = randurls[train_n:n]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLYgcz92VBhV"
      },
      "source": [
        "Funciones que modifican las imagenes de entrada para aumentar el dataset y no condicionarlo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVDgA4OjA3lI"
      },
      "source": [
        "# Dimensiones a las que se quiere reajustar la imagen, depende del tamaño de las imagenes\n",
        "width = 1024\n",
        "heigth = 1024\n",
        "# Se crea una funcion que reescale las imagenes\n",
        "def resize(inimg, heigth, width):\n",
        "  inimg = tf.image.resize(inimg, [heigth, width])   # se transforma la imagen de entrada (la que se ve mal)\n",
        "  return inimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxOIRzBCS9U"
      },
      "source": [
        "# Se crea una funcion que normalice las imagenes para que esten entre -1 y 1\n",
        "def normalize(inimg, tgimg):\n",
        "\n",
        "  inimg = (inimg/127.5) - 1  \n",
        "  tgimg = (tgimg/127.5) - 1\n",
        "\n",
        "  return inimg, tgimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaT0LRMNCr64"
      },
      "source": [
        "@tf.function()\n",
        "# Aumentacion de los datos: Random Crop + Flip, aleatoriamente se giraran algunas imagenes y de esta forma se aumentan los datos de entrada\n",
        "def random_jitter(inimg, tgimg):\n",
        "  # Se aumenta ligeramente (un 10%)\n",
        "  inimg = resize(inimg, 1144, 1144)\n",
        "  tgimg = resize(tgimg, 1144, 1144)\n",
        "  # Se ponen ambas imagenes una encima de otra\n",
        "  stacked_image = tf.stack([inimg, tgimg], axis = 0)\n",
        "  # Se mueven un poco\n",
        "  cropped_image = tf.image.random_crop(stacked_image, size = [2, heigth, width, 3])\n",
        "  # Se separan\n",
        "  inimg, tgimg = cropped_image[0], cropped_image[1]\n",
        "  # El 50% de las veces se hara un flip\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    inimg = tf.image.flip_left_right(inimg)\n",
        "    tgimg = tf.image.flip_left_right(tgimg)\n",
        "  \n",
        "  return inimg,tgimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNYKXazkGAh_"
      },
      "source": [
        "# Funcion que carga las imagenes\n",
        "def load_images(filename, augment = True):\n",
        "  \n",
        "  inimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(INPATH + '/' + filename)), tf.float32)[...,:3]\n",
        "  tgimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(OUPATH + '/' + filename)), tf.float32)[...,:3]\n",
        "\n",
        "  inimg = resize(inimg, 1024, 1024)\n",
        "  tgimg = resize(tgimg, 1024, 1024)\n",
        "\n",
        "  if augment:\n",
        "    inimg, tgimg = random_jitter(inimg, tgimg)\n",
        "\n",
        "  inimg, tgimg = normalize(inimg, tgimg)\n",
        "\n",
        "  return inimg, tgimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lInr9lQGPQj9"
      },
      "source": [
        "def load_train_image(filename):\n",
        "  return load_images(filename,True)\n",
        "def load_test_image(filename):\n",
        "  return load_images(filename,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVLCmqT8Hes2"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(tr_urls)\n",
        "train_dataset = train_dataset.map(load_train_image, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(1)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(ts_urls)\n",
        "test_dataset = test_dataset.map(load_test_image, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAfMvfDUUJTQ"
      },
      "source": [
        "# **Modelo Pix2Pix**\n",
        "\n",
        "Este modelo sigue el modelo GAN en el que dos redes neuronales compiten entre ellas siendo una la que genera imagenes falsas y otra la que decide si son reales o falsas.\n",
        "\n",
        "El generador tiene una arquitectura enconder-decoder en el que primero se comprimira la informacion de las images y luego se ira descomprimiendo. \n",
        "\n",
        "**ENCODER**\n",
        "\n",
        "El enconder tiene bloque que seran Covolucion-BatchNormalization-LeakyReLU estos bloques el paper los denomina \"C\" y les acompañará un numero en funcion del numero de filtros. La sucesion de bloques que forma el encoder es la siguiente:\n",
        "\n",
        "C64-C128-C256-C512-C512-C512-C512-C512\n",
        "\n",
        "Ademas de esto, el paper indica una serie de \"modificiciones\" para las capas:\n",
        "\n",
        "\n",
        "*   A la primera capa (C64) no se le aplicará BatchNorm\n",
        "*   Todas las convoluciones son de filtros espaciales de tamaño 4x4 con stride = 2\n",
        "*   Los pesos a la hora de inicializarse se inicializan como ruido gaussiano de media 0 y desviacion 0.02\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTPZ6li9b9vO"
      },
      "source": [
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzwVSw78xskr",
        "outputId": "a1f1895d-dced-4635-f29a-e4de158f4408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu7d4zvbYJoL"
      },
      "source": [
        "\n",
        "# Funcion que define los bloques Conv-BatchNorm-ReLU\n",
        "def downsample(filters, apply_batchnorm = True):\n",
        "  \n",
        "  result = Sequential()\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0, 0.02)\n",
        "  # Capa convolucional\n",
        "  result.add(Conv2D(filters,\n",
        "                    kernel_size = 4,\n",
        "                    strides = 2,\n",
        "                    padding = \"same\",\n",
        "                    kernel_initializer = initializer,\n",
        "                    use_bias = not apply_batchnorm))\n",
        "  if apply_batchnorm:\n",
        "    # Capa de BatchNormalization\n",
        "    result.add(BatchNormalization())\n",
        "  # Capa de Activacion Leaky ReLU\n",
        "  result.add(LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3uVRVYeVbI"
      },
      "source": [
        "**DECODER**\n",
        "\n",
        "El decoder tiene bloque que seran Covolucion-BatchNormalization-Dropout-ReLU con un dropout del 50%. Estos bloques el paper los denomina \"CD\" y les acompañará un numero en funcion del numero de filtros. La sucesion de bloques que forma el encoder es la siguiente:\n",
        "\n",
        "CD512-CD512-CD512-C512-C256-C128-C64\n",
        "\n",
        "Ademas de esto, el paper indica una serie de \"modificiciones\" para las capas:\n",
        "\n",
        "\n",
        "*   A la primera capa (C64) no se le aplicará BatchNorm\n",
        "*   Todas las convoluciones son de filtros espaciales de tamaño 4x4 con stride = 2\n",
        "*   Los pesos a la hora de inicializarse se inicializan como ruido gaussiano de media 0 y desviacion 0.02"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRQKESsJeyyu"
      },
      "source": [
        "# Funcion que define los bloques Conv-BatchNorm-ReLU\n",
        "def upsample(filters, apply_dropout = False):\n",
        "  \n",
        "  result = Sequential()\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0, 0.02)\n",
        "  # Capa convolucional\n",
        "  result.add(Conv2DTranspose( filters,\n",
        "                              kernel_size = 4,\n",
        "                              strides = 2,\n",
        "                              padding = \"same\",\n",
        "                              kernel_initializer = initializer,\n",
        "                              use_bias = False))\n",
        "  # Capa de BatchNormalization\n",
        "  result.add(BatchNormalization())\n",
        "  if apply_dropout:\n",
        "    # Capa de Dropout\n",
        "    result.add(Dropout(0.5))\n",
        "  # Capa de Activacion Leaky ReLU\n",
        "  result.add(ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu9zMtO4h6LG"
      },
      "source": [
        "**GENERADOR**\n",
        "\n",
        "El generador sigue el modelo U-NET en el trabajn conjuntamente el econder y el decoder\n",
        "\n",
        "\n",
        "![texto alternativo](https://qph.fs.quoracdn.net/main-qimg-78a617ec1de942814c3d23dab7de0b24)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljc33MgVgIYR"
      },
      "source": [
        "def Generator():\n",
        "  # Se especifica la capa de entrada especificandole las dimensiones, no se especifican el alto y ancho pero si los canales de color\n",
        "  inputs = tf.keras.layers.Input(shape = [None, None, 3])\n",
        "  # Se especifica la lista de bloques del encoder\n",
        "  down_stack = [\n",
        "               downsample(64, apply_batchnorm = False),   # (bs, 128, 128, 64)        # Esto muestra mas o menos la dimensiones (batch_size, height, width, mapas de caracteristicas)\n",
        "               downsample(128),                           # (bs, 64, 64, 128)\n",
        "               downsample(256),                           # (bs, 32, 32, 256)\n",
        "               downsample(512),                           # (bs, 16, 16, 512)\n",
        "               downsample(512),                           # (bs, 8, 8, 512)\n",
        "               downsample(512),                           # (bs, 4, 4, 512)\n",
        "               downsample(512),                           # (bs, 2, 2, 512)\n",
        "               downsample(512),                           # (bs, 1, 1, 512)\n",
        "  ]\n",
        "  # Se especifica la lista de bloques del decoder\n",
        "  up_stack = [\n",
        "              upsample(512, apply_dropout = True),        # (bs, 2, 2, 1024)\n",
        "              upsample(512, apply_dropout = True),        # (bs, 4, 4, 1024)\n",
        "              upsample(512, apply_dropout = True),        # (bs, 8, 8, 1024)\n",
        "              upsample(512),                              # (bs, 16, 16, 1024)\n",
        "              upsample(256),                              # (bs, 32, 32, 512)\n",
        "              upsample(128),                              # (bs, 64, 64, 256)\n",
        "              upsample(64),                               # (bs, 128, 128, 128)\n",
        "              upsample(32),                               # (bs, 256, 256, 64)\n",
        "              upsample(16)                                # (bs, 512, 512, 32)      \n",
        "  ]\n",
        "  # Se crea la capa final que formará la imagen\n",
        "  initializer = tf.random_normal_initializer(0, 0.02)\n",
        "  last = Conv2DTranspose(filters = 3,\n",
        "                         kernel_size = 4,\n",
        "                         strides = 2,\n",
        "                         padding = \"same\",\n",
        "                         kernel_initializer = initializer,\n",
        "                         activation = \"tanh\")\n",
        "  \n",
        "  # Se conectan las capas del encoder\n",
        "  x = inputs\n",
        "  skips = []\n",
        "\n",
        "  concat = Concatenate()\n",
        "\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "  \n",
        "  # Se conectan las capas del decoder\n",
        "  for up, sk in zip(up_stack, skips):\n",
        "\n",
        "    x = up(x)\n",
        "    x = concat([x, sk])\n",
        "  # Se pasa por la ultima capa que es la que forma la imagen\n",
        "  last = last(x)\n",
        "\n",
        "  return Model(inputs = inputs, outputs = last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z18UCNvtR-W1"
      },
      "source": [
        "def resize(inimg, heigth, width):\n",
        "\n",
        "  inimg = tf.image.resize(inimg, [heigth, width])   # se transforma la imagen de entrada (la que se ve mal)\n",
        "\n",
        "  return inimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73rt4oz2RoOx"
      },
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, to_file= \"model.png\", show_shapes=True, show_layer_names= True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J327zJhtEErC"
      },
      "source": [
        "**DISCRIMINADOR**\n",
        "\n",
        "El discriminador que presenta el paper no es de los normales, es decir no devulve un escalar indicando si la imagen generada es real o fake comparandola con una imagen real, sino que devuelve una especia de mapa en el que se indica que partes de la imagen son reales y cuales no.\n",
        "\n",
        "El discriminador sigue la siguiente arquitectura\n",
        "C64-C128-C256-C512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzta0kXcg3Rt"
      },
      "source": [
        "def Discriminator():\n",
        "  # El discriminador recibe dos entradas la imagen original y la generada\n",
        "  ini = Input(shape=[None, None, 3], name = \"input_img\")\n",
        "  gen = Input(shape=[None, None, 3], name = \"gener_img\")\n",
        "  # Las va a concatener y poner una encima de la otra\n",
        "  con = concatenate([ini, gen])\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0, 0.02)\n",
        "\n",
        "  down1 = downsample(64, apply_batchnorm = False)(con)\n",
        "  down2 = downsample(128)(down1)\n",
        "  down3 = downsample(256)(down2)\n",
        "  down4 = downsample(512)(down3)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(filters = 1,\n",
        "                                kernel_size = 4,\n",
        "                                strides = 1,\n",
        "                                kernel_initializer = initializer,\n",
        "                                padding = \"same\")(down4)\n",
        "  \n",
        "  return tf.keras.Model(inputs = [ini, gen], outputs = last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDeBZ81vR0P1"
      },
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfRSwkYVHNY7"
      },
      "source": [
        "**FUNCIONES DE COSTE**\n",
        "\n",
        "El `real_loss`evalua la diferencia entre el resultado del discriminador al observar una imagen real comparandolo con el resultado idoneo es decir una imagen 100% real que seria una matriz a 1s\n",
        "\n",
        "El `generated_loss` lo que esta haciendo es comprobar el resultado del discriminador al observar la imagen que ha sido generada por el Generador y compararla con una imagen 100% falsa que seria una matriz de 0s\n",
        "\n",
        "Estas dos componentes evaluan si el trabajo del Discriminador es el adecuado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdg41BcYG374"
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    # Diferencia entre los true por ser real y el deectado por el discriminador\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    # Diferencia entre los false por ser generado y el detectado por el discriminador\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    total_loss = real_loss + generated_loss\n",
        "    \n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3NCXM8oIxBy"
      },
      "source": [
        "El generador tiene dos objetivos, uno de ellos es generar una imagen realista, y su otro objetivo es conseguir qeu el error del discriminador se maximice. Por eso se le pasa la imagen que ha generado `gen_output`, la imagen objetivo `target` y el mapa que ha generado el discriminador al observar su imagen `disc_generated_output`\n",
        "\n",
        "Se computan dos errores diferentes, el adversario `gan_loss` y la diferencia absoluta por pixeles `l1_loss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp9K98hCIxbE"
      },
      "source": [
        "LAMBDA = 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA + l1_loss)\n",
        "\n",
        "  return total_gen_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3sUI9T7MK9O"
      },
      "source": [
        "**OPTIMIZADORES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy6xD6DKMH2e"
      },
      "source": [
        "# Optimizadores con los hiperparamtros que especifica el paper\n",
        "generator_optimizer     = tf.keras.optimizers.Adam(2e-4, beta_1 = 0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1 = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBONLnXKMebV"
      },
      "source": [
        "**CHECKPOINT DEL ENTRENAMIENTO**\n",
        "\n",
        "Con esto si Google Colab se cierra o tiene un error, como se van guardando los optimizadores y el proceso del entrenamiento pues no hay que empezar de cero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gheKV9eMjdP"
      },
      "source": [
        "import os\n",
        "checkpoint_prefix = os.path.join(CKPATH, \"chpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,\n",
        "                                 discriminator_optimizer = discriminator_optimizer,\n",
        "                                 generator = generator,\n",
        "                                 discriminator = discriminator)\n",
        "\n",
        "#checkpoint.restore(tf.train.latest_checkpoint(CKPATH)).assert_consumed()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMxdsotQO76b"
      },
      "source": [
        "**EVALUACION DEL MODELO DURANTE EL ENTRENAMIENTO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59tvMh_ZO5Nq"
      },
      "source": [
        "def generate_images(model, test_input, tar, epoch, save_filename = False, display_images = True):\n",
        "  prediction = model(test_input, training=True)\n",
        "\n",
        "  if save_filename and epoch % 10 == 0:\n",
        "    tf.keras.preprocessing.image.save_img(PATH_RESULTS + '/Predicted/' + save_filename + '_pre.jpg', prediction[0,...])\n",
        "  if save_filename and epoch == 0:\n",
        "    tf.keras.preprocessing.image.save_img(PATH_RESULTS + '/Target/' + save_filename + '_tar.jpg',tar[0,...])\n",
        "    tf.keras.preprocessing.image.save_img(PATH_RESULTS + '/Input/' + save_filename + '_input.jpg',test_input[0,...])\n",
        "\n",
        "  plt.figure(figsize=(15,15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  if display_images:\n",
        "    for i in range(3):\n",
        "      plt.subplot(1, 3, i+1)\n",
        "      plt.title(title[i])\n",
        "      # getting the pixel values between [0, 1] to plot it.\n",
        "      plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3w4wS6QPdPL"
      },
      "source": [
        "# **ENTRENAMIENTO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjn1y0CN1Lr_"
      },
      "source": [
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy4SGsmBRsVZ"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(input_image, target, epoch):\n",
        "  \n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    # Generador\n",
        "    output_image = generator(input_image, training = True)\n",
        "    # Discriminador\n",
        "    output_gen_disc = discriminator([output_image, input_image], training = True)\n",
        "    output_trg_disc = discriminator([target, input_image], training = True)\n",
        "    # Funciones de pérdida\n",
        "    disc_loss = discriminator_loss(output_trg_disc, output_gen_disc)\n",
        "    gen_loss   = generator_loss(output_gen_disc, output_image, target)\n",
        "    # Gradientes\n",
        "    generator_grads    = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    discrminator_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    # Optimizador\n",
        "    generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discrminator_grads, discriminator.trainable_variables))\n",
        "    with summary_writer.as_default():\n",
        "      tf.summary.scalar('gen_total_loss', gen_loss, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCbq0YldPkIP"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def train(dataset, epochs):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    imgi = 0\n",
        "    for input_image, target in dataset:\n",
        "      print('epoch ' + str(epoch) + '- train: ' + str(imgi) + '/' + str(len(tr_urls)))\n",
        "      imgi += 1\n",
        "      train_step(input_image, target, epoch)\n",
        "      clear_output(wait = True)\n",
        "    \n",
        "    imgi = 0\n",
        "    for inp, tar in test_dataset.take(5):\n",
        "      generate_images(generator, inp, tar, epoch, str(imgi) + '_' + str(epoch), display_images = True)\n",
        "      imgi += 1\n",
        "    # Guardar (checkpoint) el modelo cada 20 epocas\n",
        "    if(epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    if(epoch + 1) % 20 == 0:\n",
        "      generator.save('') # save model path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm8nkSr1wKJm",
        "outputId": "0f31b40e-62e5-4c6b-bc66-c52b1614665f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Uncomment if you want restore a checkpoint\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(CKPATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4905101cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ODwKFIEQ4uD"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "\n",
        "\n",
        "train(train_dataset, 61)\n",
        "generator.save('/content/drive/My Drive/DeepLearning/recons+sr/Model/recons_sr_GAN_finish.h5')\n",
        "print(\"Finish training\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}